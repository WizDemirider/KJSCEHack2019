

PageBreak

Contents

Preface

1 Introduction

2 Statistical Learning
2.1 What Is Statistical Learning? ... 2... 2. 2. ee

Qu
2.1.2
2.1.3

2.1.
2.1.5

2.2 Assessing Model Accuracy.........

2.2%
2.2.2
2.2.3
2.3 Lab:
2.3,
2.3.2
2:39
2.3%
2.3.5

 

Introduction tO Rens van van 2 ¢

Why Estimate f?... 0.2.0...
How Do We Estimate f? .......0.0.0000.
The Trade-Off Between Prediction Accuracy

and Model Interpretability ..............
Supervised Versus Unsupervised Learning ......

  

Regression Versus Classification Problems ..... .

  
  

Measuring the Quality of Fit
The Bias-Variance Trade-Off
The Classification Setting .....

Basic Commands ...............0000.5
Graphics 2... ee
Tndexatie Dati: «2 ees gua eye a owe Bw ERS
Loading Data... 2... 2.2 ee
Additional Graphical and Numerical Summaries

24 ExXOrGIS68! ¢ os yee ESS RWS SE ES RR RE RWS Ems

vii

15
15
17
21

24
26
28
29
29
33
37
42
42
45
AT
48
49
52

ix

PageBreak

x Contents

 

  

  

3 Linear Regression 59
3.1 Simple Linear Regression ......... 61
3.1.1 Estimating the Coefficients 61
3.1.2 Assessing the Accuracy of the Coefficient
Estimates! 2: sas ees eee tees HME BWA Ewe 63
3.1.3 Assessing the Accuracy of the Model... ...... 68
3.2 Multiple Linear Regression ............-..00- 71
3.2.1 Estimating the Regression Coefficients ........ 72
3.2.2 Some Important Questions .............. 75
3.3 Other Considerations in the Regression Model... .... . 82
3.3.1 Qualitative Predictors .... 2.0.2.0... 000004 82
3.3.2 Extensions of the Linear Model... . 2.2.2... 86
3.3.3 Potential Problems... .............0-5. 92
3.4 The Marketing Plan... 2... 2... ... 20.00.0040. 02
3.5 Comparison of Linear Regression with K-Nearest
Neighbors: ci 2 sis eae ems ee eee RAY Hee EER 04
3.6 Lab: Linear Regression... 2.2... 0 ee ee 09
36:1 Libraries: 1 sas ses sae toa es ERE Ew E EHS 09
3.6.2 Simple Linear Regression ..............2- 10
3.6.3 Multiple Linear Regression .............. 13
3.6.4 Interaction Terms ...........0. 000085 15
3.6.5 Non-linear Transformations of the Predictors .... 115
3.6.6 Qualitative Predictors .............000.5 17
3.6.7 Writing Functions ... 2.0.0.0... . 00008. 19
3.7 Exercises 2... ee 20
4 Classification 127
4.1 An Overview of Classification»... 0.0... 00000004 28
4.2 Why Not Linear Regression? ..............00- 29
4.3 Logistic Regression... 2... ee 30
4.3.1 The Logistic Model... ........ 0.000085 31
4.3.2 Estimating the Regression Coefficients ....... . 33
4.3.3 Making Predictions. .............0.05- 34
4.3.4 Multiple Logistic Regression... ........... 35
4.3.5 Logistic Regression for >2 Response Classes... . . 37
4.4 Linear Discriminant Analysis .........0.....004 38
4.4.1 Using Bayes’ Theorem for Cl. i 38
4.4.2 Linear Discriminant Analys 39
4.4.3 Linear Discriminant Analysis forp>1........ 42
4.4.4 Quadratic Discriminant Analysis ........... 49
4.5 A Comparison of Classification Methods ........... 51
4.6 Lab: Logistic Regression, LDA, QDA, and KNN ...... 54
4.6.1 The Stock Market Data ............. 048.4 54
4.6.2 Logistic Regression... 2... ee ee 56
4.6.3 Linear Discriminant Analysis ............. 61

PageBreak

Contents xi

 

4.6.4 Quadratic Discriminant Analysis ........... 63
4.6.5  K-Nearest Neighbors... ............00. 63
4.6.6 An Application to Caravan Insurance Data ..... 65
4.7 Exercises 2... ee 68
Resampling Methods 175
5.1 Cross-Validation 2... 0. ee ee 76
5.1.1 The Validation Set Approach ...........00. 76
5.1.2 Leave-One-Out Cross-Validation ........... 78
5.1.3 k-Fold Cross-Validation .............00. 81
5.1.4 Bias-Variance Trade-Off for k-Fold
Cross-Validation . 2... 0.000000. e eee 83
5.1.5 Cross-Validation on Classification Problems .... . 84
5.2 The.Bootstrap sa. emu wwn ewe s wae me Hae oS 87
5.3 Lab: Cross-Validation and the Bootstrap. .......... 90
5.3.1 The Validation Set Approach ............. 91
5.3.2 Leave-One-Out Cross-Validation ........... 92
5.3.3 k-Fold Cross-Validation .............-.. 93
5.3.4 The Bootstrap ...............22000.5 94
DA BXOPGISGS! 2.2 2 cis ee Na Be DRE Re ER Bw 97
Linear Model Selection and Regularization 203
6.1 Subset Selection 2. cei ven bene eee ewe ewe bee 205
6.1.1 Best Subset Selection ..............-0. 205
6.1.2 Stepwise Selection ...............0008.5 207
6.1.3 Choosing the Optimal Model ............. 210
6.2 Shrinkage Methods... ......0... 0.0.00 000004 214
6.2.1, Ridge Regression. «0 c05 saws wae wwe ows 215
62:2 “THe Dass a: sms ses 84k 4 HE ERE ERE Bw 219
6.2.3 Selecting the Tuning Parameter. ........... 227
6.3 Dimension Reduction Methods ..............0.. 228
6.3.1 Principal Components Regression. .......... 230
6.3.2 Partial Least Squares .............000- 237
6.4 Considerations in High Dimensions... ........... 238
6.4.1 High-Dimensional Data ................ 238
6.4.2 What Goes Wrong in High Dimensions? ...... . 239
6.4.3 Regression in High Dimensions ............ 241
6.4.4 Interpreting Results in High Dimensions. ...... 243
6.5 Lab 1: Subset Selection Methods ............... 244
6.5.1 Best Subset Selection ............0000. 244
6.5.2 Forward and Backward Stepwise Selection... ... 247

6.5.3 Choosing Among Models Using the Validation
Set Approach and Cross-Validation. ......... 248

PageBreak

xii

  

  

Contents
6.6 Lab 2: Ridge Regression and the Lasso... .........- 251
6.6.1 Ridge Regression... 2... 0.0.0 ee ee eee i
6.6.2 The Diass0 w.. cic cme aus © ae He He EO 255
6.7 Lab 3: PCR and PLS Regression ...........0 004 256
6.7.1 Principal Components Regression. .......... 256
6.7.2 Partial Least Squares ............-008- 258
6.8 Exercises 2... 0... ee 259
Moving Beyond Linearity 265
7.1 Polynomial Regression... 1.2... 0-0. eee eee 266
7.2 Step Functions .. 2... 0... ee 268
(oo BasistPunctions 6 acs sms cme eae same Ke HS ome 270
7.4 Regression Splines 2... 0. ce ees 271
7.4.1 Piecewise Polynomials ............-..05. 271
7.4.2 Constraints and Splines .............02. 271
7.4.3 The Spline Basis Representation ........... 273
7.4.4 Choosing the Number and Locations
ofthe Knots «au ams ewe eee Ra Hm HOP 274
7.4.5 Comparison to Polynomial Regression ....... . 276
7.5 Smoothing Splines ....... 2.0.00. 0000000004 277
7.5.1 An Overview of Smoothing Splines... ....... 277
7.5.2 Choosing the Smoothing Parameter A ........ 278
7.6 Loecal-Regression a. «mic ue aon 6 ae me we em 280
7.7 Generalized Additive Models ..............00. 282
7.7.1. GAMs for Regression Problems... ......... 283
7.7.2 GAMs for Classification Problems .......... 286
7.8 Lab: Non-linear Modeling ...............000.% 287
7.8.1 Polynomial Regression and Step Functions .... . 288
82 Splits: awe vers ams ve eee Ree He 293
%88 GAMSi. cus coe wee ee vy De Ome eR Ba 294
79 BxOreises:! 22s ume ESS RWS LMS EYP ER RSE Ewe ee 297
Tree-Based Methods 303
8.1 The Basics of Decision Trees ............00004 303
8.1.1 Regression Trees .. 2.2... 0 eee eee 304
8.1.2 Classification Trees... 2.2... eee 311
8.1.3 Trees Versus Linear Models .............. 314
8.1.4 Advantages and Disadvantages of Trees ...... . 315
8.2 Bagging, Random Forests, Boosting ............. 316
82:1 Bagpitig 2s: sss ees YR ee eRe HME Rw E Ewe 316
8.2.2 Random Forests .........0 00000000005 319
8.2.3 Boosting... 6... ee 321
8.3. Lab: Decision Trees . ow ue coe 6 eae ee ee me 323
8.3.1 Fitting Classification Trees ...........00. 323

8.3.2 Fitting Regression Trees... ..........-5. 327

PageBreak

Contents xiii

 

 

8.3.3 Bagging and Random Forests ............. 328
8.34, Boosting os. cain cme aus © cee He Bae EO 330
8:4 Ex€reis68: <2: yes ESS RWS RHE ESR RRE RWS Ew 332
9 Support Vector Machines 337
9.1 Maximal Margin Classifier... 2.2... 0.0.0.0. 004 338
9.1.1 What IsaHyperplane? ................ 338
9.1.2 Classification Using a Separating Hyperplane ... . 339
9.1.3 The Maximal Margin Classifier... 2.2.5.0... 341
9.1.4 Construction of the Maximal Margin Classifier .. . 342
9.1.5 The Non-separable Case... ..........00.5 343
9.2 Support Vector Classifiers... 2... 0.0.0. 20 0004 344
9.2.1 Overview of the Support Vector Classifier... .. . 344
9.2.2 Details of the Support Vector Classifier ....... 345
9.3 Support Vector Machines ................0.. 349
9.3.1 Classification with Non-linear Decision
BOUNGaLION . cnn wm vos eee HE Ha VD 349
9.3.2 The Support Vector Machine ............. 350
9.3.3 An Application to the Heart Disease Data... .. . 354
9.4 SVMs with More than Two Classes... ........000.- 355
9.4.1 One-Versus-One Classification... ........0. 355
9.4.2 One-Versus-All Classifi 356
9.5 Relationship to Logistic Regression... ..........- 356
9.6 Lab: Support Vector Machines .............-.- 359
9.6.1 Support Vector Classifier ............00. 359
9.6.2 Support Vector Machine.............-.. 363
9.6.3 ROC Curves .. 2... 0... ee 365
9.6.4 SVM with Multiple Classes ...........0.. 366
9.6.5 Application to Gene Expression Data ........ 366
9.7 Exercises 2... 2. ee 368
10 Unsupervised Learning 373
10.1 The Challenge of Unsupervised Learning. .......... 373
10.2 Principal Components Analysis... ......0....000. 374
10.2.1 What Are Principal Components? .......... 375
10.2.2 Another Interpretation of Principal Components .. 379
10:2.38. More'on PCA kei eee eee ee we ewe bee 380
10.2.4 Other Uses for Principal Components ........ 385
10.3 Clustering Methods... 2... 20. .0.0.0 2000000005 385
10.3.1 K-Means Clustering ..............000. 386
10.3.2 Hierarchical Clustering... ...........-.. 390
10.3.3 Practical Issues in Clustering ..........-.. 399

10.4 Lab 1: Principal Components Analysis ............ 401

PageBreak

xiv Contents

10.5 Lab 2: Clustering... 2.2... 2. 2 ee eee 404
10.5.1 K-Means Clustering ..............000. 404
10.5.2 Hierarchical Clustering... ...........00. 406

10.6 Lab 3: NCI60 Data Example ..............00. 407
10.6.1 PCA on the NCI60 Data ............0.0. 408
10.6.2 Clustering the Observations of the NCI60 Data ... 410

10.7 Exercises 2. 0. ee 413

Index 419